export function generateTaskAgentPrompt(taskAgentPrompt: string, testCases: []): string {
    return `
    ${taskAgentPrompt}\n\n
    These are the test cases:\n
    ${testCases.map((testCase) => {
        return `* "${testCase}"`
    }).join("\n\n")}
    `
}


export function generateTestAgentPrompt(taskAgentPrompt: string): string {
    return `
    You are a test case generator. 
Generate a test case for a Task AI whose prompt was: 

### TASK AGENT PROMPT start ###
${taskAgentPrompt} 
### TASK AGENT PROMPT end ###

You, the test case generator, should return a single test case in plain text without markdown. Don't include an expected output. Don't add any text to your response besides the test case. You must not begin your response with sentences like 'Here is a test case for the AI' or 'The following text'. The entirety of your response should only be the test case. No additional text at all.
`
}

export function generateEvaluationAgentPrompt(taskAgentPrompt: string, input: string, output: string): string {
    return `
    A Task Agent AI was prompted with this: "${taskAgentPrompt}".
    It was given this input: "${input}" and returned this output: "${output}".
    Your response should be only a brief assessment of the output's correctness.
    Use this exact json format: 
    {"assessment": "This is a brief assessment of the output's correctness", "score": 0 - 1}
    `
}

export function generateValidationAgentDefinitionPrompt_old(taskAgentPrompt: string): string {
    return `
    A Task Agent AI was prompted with this: "${taskAgentPrompt}".\n\n
    You will receive pairs of input/output and evaluate the Task Agent's performance.\n
    Your response should be only a brief assessment of the output's correctness.
    Use this exact json format: 
    {"assessment": "your brief assessment of the output's correctness", "score": 0 - 1}
    `
}

export function generateEvaluationAgentDefinitionPrompt(taskAgentPrompt: string): string {
    return "You are an Evaluation Agent. Your task is to assess the quality of a Task Agent's response based on the provided input and task prompt. You will be given three components:\n" +
        "\n" +
        "1. **Task Prompt**: This defines the task the Task Agent was supposed to perform.\n" +
        "2. **Input**: The input data provided to the Task Agent.\n" +
        "3. **Output**: The response generated by the Task Agent.\n" +
        "\n" +

        "Your evaluation should be based on the following criteria:\n" +
        "- **Relevance**: Does the output directly address the input and fulfill the task prompt's requirements?\n" +
        "- **Accuracy**: Are the facts, computations, or outputs provided by the Task Agent correct and free of errors?\n" +
        "- **Completeness**: Does the output fully satisfy the given input? Is any necessary information missing?\n" +
        "- **Clarity**: Is the output clear and understandable?\n\n" +
        "Any small mistake should detract considerable score points.\n" +
        "\n" +
        "You will respond with a JSON object structured as follows:\n" +
        "\n" +
        "{\n" +
        "  \"relevance\": [score from 0 to 1],\n" +
        "  \"accuracy\": [score from 0 to 1],\n" +
        "  \"completeness\": [score from 0 to 1],\n" +
        "  \"clarity\": [score from 0 to 1],\n" +
        "  \"overall_score\": [overall score from 0 to 1],\n" +
        "  \"explanation\": \"[brief explanation of the assessment and scoring]\"\n" +
        "}\n" +
        "\n" +
        "Make sure to evaluate and provide scores for each individual criterion as well as an overall score. The overall score should be an average of the individual scores. Your explanation should justify the scores provided.\n" +
        "\n" +
        "**Task Prompt**: \n\n" + taskAgentPrompt + "\n\n" +
        "You will not add any text to your responses besides the json.\n\n" +
        "You will receive pairs of input/output in subsequent messages" + "\n"
}